\chapter{Introduction} \label{chp:Introduction}
In this first chapter we want to focus on introducing the main aspects of this thesis. We will start by giving some more motivation and historical context on reinforcement learning and our particle navigation problem in Section \ref{sec:Motivation}. We further give an overview over related work in Section \ref{sec:RelatedWork}. Finally, we will go over a short summary of our own results and key findings in Section \ref{sec:Results}.  

\section{Motivation} \label{sec:Motivation}
\paragraph{History on Machine Learning.}
The idea of intelligent machines which are able to learn more or less autonomously, which are able to solve any complex task by themselves and which thrive for human capabilities or even exceed them was theorized and discussed long before computers were able to learn anything at all. For a long time fiction was way ahead of reality, with films and novels drawing a future of human-like robots - be it positive or negative - while in reality machine learning still struggled at the easiest tasks and was computationally way too heavy to ever run on any machine in a real-time scenario. While we all dreamed of our own Wall-E or feared the rise of Skynet the birth of modern computing not only inspired authors, but also inspired researchers to explore self-learning computers.

 This wish to develop intelligent machines found its first success in 1943, when McCulloch and Pitts described how neural networks can be used to build mathematical models for logical or arithmetic expressions \cite{mcculloch1943logical}. Their work was picked up over a decade later and lead to the birth of artificial neural networks in 1959, which were introduced by Frank Rosenblatt in the form of perceptrons \cite{rosenblatt1958perceptron}. While the capabilities of perceptrons were very limited, they demonstrated that machines were able to do more than static computations which created a whole new field of computer science.

 Even though we had the building blocks to artificial neural networks, there was still a long way before they became as powerful as they are today. Researchers were alternating between breakthroughs and decades of standstill. Only 20 years ago most scientists agreed, that artificial neural networks will never become as powerful as other machine learning technologies with stronger mathematical foundations like support vector machines. Today, after the introduction of non-linearity to perceptrons, the breakthrough of the backpropagation algorithm and countless other advances, state-of-the-art models are capable of surpassing humans in a lot of complex tasks like handwriting recognition and are used to solve all kinds of problems where traditional algorithms fail to deliver results. Artificial neural networks developed beyond a point where their capabilities outclass most of the other competing techniques for machine learning and with the ever rising computational power their capabilities only grow further. In chapter \ref{chp:DeepLearning} we will talk in detail about how modern neural networks work and which techniques can be used to train them.

 \paragraph{Learning by Doing: Reinforcement Learning} In this work we will be using techniques from a specific field of machine learning called reinforcement learning or more specifically its modern combination with artificial neural networks called Deep Reinforcement Learning (also \textit{DRL} or Deep RL).

 Deep RL is one of the most exciting fields of machine learning today, because it is capable of achieving actual superhuman performance without any human ever creating training data for it. In reinforcement learning the data is created solely by an agent interacting with its environment. Reinforcement learning can therefore be compared to how humans learn themselves - by trial and error. Deep RL algorithms therefore are unsupervised machine learning algorithms.

 Like traditional machine learning, reinforcement learning has been around for decades, but just recently gained notable attention. In 2013 researchers from the British startup DeepMind were able to train a system to play any game from the game console Atari without prior knowledge and only with raw pixel data as input. \cite{mnih2013playing} They later even improved their system and were able to outperform trained human players \cite{mnih2015human}.

 But these two successes were only the beginning of a series of advances for deep reinforcement learning. After Google acquired DeepMind their new system \textit{AlphaGo} was able to defeat one of the worlds top class Go players Lee Sedol with 4:1 \cite{borowiec2016alphago} and even the world champion Ke Jie in 2017. The system was then expanded and generalized to also play shogi and chess and renamed to \textit{AlphaGo Zero}. AlphaGo Zero not only defeated its predecessor, but also defeated state-of-the-art alpha-beta search engines for chess like Stockfish \cite{silver2017mastering}. In 2019 they reached a new milestone, when their system \textit{AlphaStar} was able to beat a professional player at StarCraft II - a complex multiplayer strategy game \cite{arulkumaran2019alphastar}. 

 All these advances were made possible by a number of advances in machine learning and reinforcement learning as well as an increase computational power. We will take an in-depth look on current deep reinforcement learning techniques and how they work in Chapter \ref{chp: RLOverview}.

 \paragraph{Everything can be a Game.}
 We saw reinforcement learning had great success for games, but what happens if we want to solve more abstract problems like wayfinding? Can we still apply these algorithms? The short answer is yes: If we just change our perspective, most algorithmic problems can be easily transformed into a game (and most games into algorithmic problems). We always have something we can express as a state - be it as an actual image or just some numbers - and a well-defined objective for the "player", like finding the shortest path between two points. The player generates the solution for our problem step by step by playing a single round of the "game".

 In this work we want to tackle a specific problem: Navigating a swarm of particles to a goal position in a maze-like environment, by applying a global uniform force. This problem finds its application in medicine in form of targeted drug delivery. The goal is to localize medical treatment to efficiently combat cancer, localized infections or internal bleeding, without causing unwanted and potentially harmful side effects for the rest of the body. The delivery requires careful navigation of the distributed microscopic particles through pathways of blood vessels to a target location. As the particles are too small to build microrobots with sufficient energy to swim against flowing blood, a global external force like an electromagnetic field is used for motion control. This means all particles are subjected into the same direction unless their path is blocked by obstacles. Since all the particles start in different locations, navigating all particles by a uniform force to a single destination is notoriously hard.

 In this work, we want to explore the capability of modern reinforcement learning algorithms to solve this navigation problem. In 2020 it was proven by Becker et al. \cite{becker2020} that the navigation problem is NP-hard. This means, that approximation algorithms will be very important for any practical application. Becker et al. also showed, that it is possible to solve certain instances of the problem using reinforcement learning and that the resulting solution outperformed current approximation algorithms. While their approach delivered a proof-of-concept, it was computationally very heavy. With this prior knowledge we want to speed up the learning process, aim for larger instances and investigate the possibilities of reinforcement learning to deal with extended settings which are closer to real-world targeted drug delivery. Our results may also be useful for the application of reinforcement learning on other algorithmic problems.

\section{Related Work} \label{sec:RelatedWork}
In this thesis, we want to use reinforcement learning, to train neural networks to steer large numbers of microrobots or microscopic particles through a maze-like environment to a goal position. In a real-world scenario this maze-like environment may be the blood vessels in a human body, and the target position may be a tumor. 

To date, we already have the possibility to create particles which contain a magnetic core and have a catalytic or biodegradable surface which allows them to release their payload at the target position \cite{litvinov2012high, mellal2015magnetic}. To navigate these particles through the blood vessels, magnetic fields produced by modified gradient coils inside of an MRI scanner can be used \cite{mathieu2007magnetic, mathieu2010steering}. The scanner can also be used to provide real-time feedback and track the position of the nanoparticles \cite{pouponneau2009magnetic}. Since blood vessels build a complex system of paths, directing the particles to the goal position requires efficient motion planning algorithms. 

Controlling a larger number of particles by global inputs has been studied in very different settings in the past. Assembling complex shapes using a swarm of particles was explored in \cite{becker2018tilt} and \cite{balanza2019full}. Often it is not required to bring the particles to a goal position, but to rearrange them in a certain way, which has been done in \cite{becker2013massive} and \cite{zhang2017rearranging}. 

Moving particles to a goal position is directly related to another problem studied in the past called \textit{rendezvous search}. Rendezvous search aims at finding a sequence of movements for two or more independent agents to meet at the same location. Since it is easy to move a single particle (or a number of particles concentrated at a single point) to the goal position via the shortest available path, we can express our problem by a sequence of rendezvous between particles, ultimately gathering all particles at a single location.

 Early algorithmic approaches and settings were introduced by Alpern and Gal \cite{alpern2006theory} as well as by Anderson and Fekete \cite{anderson2001two}. Their work included agents with limited onboard computations, but in certain cases both agents followed the same movement protocol - executing the same movement. The problem can therefore also be called \textit{symmetric} rendezvous search where the symmetry is only broken by interaction with obstacles. Since all agents perform the same movements, this directly corresponds to the particle navigation problem. 

Rendezvous search was extended into a sequence of rendezvous to collect a swarm of particles in a single place by Mahadev et al. \cite{mahadev2016collecting}. Just recently it was proven, that computation of an optimal gathering sequence is NP-hard \cite{becker2020}. The authors also proposed several new approximation algorithms, including an approach for the application of reinforcement learning \cite{huang2019, becker2020}. Even though the reinforcement learning approach is computationally more expensive, it produces much shorter gathering sequences.

In their approach, Becker et al. used recent techniques to train the RL agent. To improve training, they included an intrinsic reward signal, guiding the agent to explore "interesting" environment states. This technique was explored for a long time to supplement sparse extrinsic rewards \cite{pathak2017curiosity, mohamed2015variational, houthooft2016variational} and recently found notable attention with the introduction of the \textit{intrinsic curiosity module} \cite{burda2018large} and the \textit{random network distillation} methods \cite{burda2018exploration}. The latter uses prediction error to simulate real intrinsic motivation. Both techniques have shown to drastically improve performance on "hard" games from the Atari game suite when used in combination with the well-known trust region optimization technique \textit{proximal policy optimization} \cite{schulman2017proximal}. Just recently, Baida et al. presented an improved curiosity signal which has shown to work exceptionally well even in the presence of extremely noise maze-like environments \cite{badia2020never}.

\section{Our Results} \label{sec:Results}
Hopefully we have some by the time.