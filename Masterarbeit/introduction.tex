\chapter{Introduction} \label{chp:Introduction}
In this chapter we want to give some introduction into the main focus of this thesis. We start by giving some motivation and historical context on reinforcement learning in Section \ref{sec:Motivation}. We further give an overview over related work in Section \ref{sec:RelatedWork}, while giving a short summary of our own results in Section \ref{sec:Results}.  

\section{Motivation} \label{sec:Motivation}
\paragraph{History on Machine Learning.}
The idea of intelligent machines which are able to learn more or less autonomously, which are able to solve any complex task by themselves and which thrive for human capabilities or even exceed them was theorized and discussed long before computers were able to learn anything at all. For a long time fiction was way ahead of reality, with films and novels drawing a future of human-like robots - be it positive or negative - while in reality machine learning still struggled at the easiest tasks and was computationally way too heavy to ever run on any machine in a real-time scenario. While we all dreamed of our own Wall-E or feared the rise of Skynet the birth of modern computing not only inspired authors, but also inspired researchers to explore self-learning computers.

 This wish to develop intelligent machines found its first success in 1943, when McCulloch and Pitts described how neural networks can be used to build mathematical models for logical or arithmetic expressions \cite{mcculloch1943logical}. With the birth of artificial neural networks, which were introduced in 1959 by Frank Rosenblatt in the form of perceptrons \cite{rosenblatt1958perceptron} a new field of computer science was born.

 Even though we had the building blocks to artificial neural networks, there was still a long way before they became as powerful as they are today. Researchers were alternating between breakthroughs and decades of standstill. Even 20 years ago most scientists agreed that artificial neural networks will never become as powerful as other machine learning technologies with stronger mathematical foundations like support vector machines. Today, after the introduction of non-linearity to perceptrons, the breakthrough of the backpropagation algorithm and countless other advances, state-of-the-art models are capable of surpassing humans in a lot of complex tasks like handwriting recognition and are used to solve all kinds of problems where traditional algorithms fail to deliver results. Artificial neural networks developed beyond a point where their capabilities outclass most of the other competing techniques for machine learning and with the ever rising computational power their capabilities only grow further.

 \paragraph{Learning by Doing: Reinforcement Learning} In this work we will be using techniques from a specific sector of machine learning called reinforcement learning or more specifically its modern combination with artificial neural networks called Deep Reinforcement Learning (also \textit{DRL} or Deep RL).

 Deep RL is one of the most exciting fields of machine learning today, because it is capable of achieving actual superhuman performance without any human ever creating any training data for it. In reinforcement learning the data is created solely by an agent interacting with its environment. Reinforcement learning can therefore be compared to how humans learn themselves. Therefore all Deep RL algorithms are unsupervised machine learning algorithms.

 Like traditional machine learning, reinforcement learning has been around for decades, but just recently gained notable attention. In 2013 researchers from the British startup DeepMind were able to train a system to play any game from the game console Atari without prior knowledge and only with raw pixel data as input. \cite{mnih2013playing} They later even improved their system and were able to outperform trained human players \cite{mnih2015human}.

 But these two successes were only the beginning of a series of advances for deep reinforcement learning. After Google acquired DeepMind their new system \textit{AlphaGo} was able to defeat one of the worlds top class Go players Lee Sedol with 4:1 \cite{borowiec2016alphago} and even the world champion Ke jie in 2017. The system was then expanded and generalized to also play shogi and chess and renamed to \textit{AlphaGo Zero}. AlphaGo Zero not only defeated its predecessor, but also defeated state-of-the-art alpha-beta search engines for chess like Stockfish \cite{silver2017mastering}. In 2019 they reached a new milestone, when their system \textit{AlphaStar} was able to beat a professional player at StarCraft II - a complex multiplayer strategy game \cite{arulkumaran2019alphastar}.

We will take an in-depth look of current deep reinforcement learning techniques and how they work in Chapter \ref{chp: RLOverview}.

 \paragraph{Everything can be a Game.}
 We saw reinforcement learning had great success for games, but what happens if we want to solve more abstract problems like wayfinding? Can we still apply these algorithms? The short answer is yes: If we look at it, most algorithmic problems can be easily transformed into a game. We always have something we can express as a state - be it as an actual image or just some numbers - and a well-defined objective for the "player", like finding the shortest path between two points. The player generates the solution for our problem step by step by playing a single round of the "game".

 In this work we want to tackle a specific problem: Navigating a swarm of particles to a goal position in a maze-like environment, by applying a global uniform force. This problem finds its application in medicine in form of targeted drug delivery. The goal is to localize medical treatment to efficiently combat cancer, localized infections or internal bleeding, without causing unwanted and potentially harmful side effects for the rest of the body. The delivery requires navigation of the distributed microscopic particles through pathways of blood vessels to a target location. As the particles are too small to build microrobots with sufficient energy to swim against flowing blood, a global external force like an electromagnetic field is used for motion control. This means all particles are subjected into the same direction unless their path is blocked by obstacles. Since at the start all the particles are in different locations, navigating all particles by a uniform force to a single destination is not an easy task.

 In this work, we want to explore the capability of modern reinforcement learning algorithms on solving this problem. As previous work \cite{becker2020} showed that the problem is NP-hard, finding optimal solutions will not be the goal. Instead we will compare the results to state-of-the-art approximation algorithms. Becker et al. also showed, that reinforcement learning is capable of solving this problem for small instances in relatively short time. With this prior knowledge we want to speed up the learning process, aim for larger instances and investigate the possibilities of transfer learning for targeted drug delivery. Our results may also be useful for the application of reinforcement learning on other algorithmic problems.

 \section{Related Work} \label{sec:RelatedWork}
 Some related work. Many nice sources. \\
 Two parts: RL and targeted drug delivery \\
 RL: Large Scale Study, PhD Thesis/Paper
 TDD: Papar, Details on tdd


\section{Our Results} \label{sec:Results}
Hopefully we have some by the time.