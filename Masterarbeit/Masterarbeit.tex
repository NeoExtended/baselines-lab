\documentclass[%
  a4paper,%
  11pt,% <10pt, 9pt>
  style=print,
  %sender=bottom,
  blue,% <orange, green, violet>
  %rgb, <cmyk>
  %mono,
  bibliography=totoc,
  nexus,
  lnum,
  extramargin,
  table
  ]{tubsbook}

\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{svg}
\usepackage[hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage[makeroom]{cancel}
\usepackage[ruled, linesnumbered, algosection]{algorithm2e}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{threeparttable}
\usepackage{dcolumn}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{booktabs}
\newcolumntype{L}{D{.}{.}{2,2}}
\makeatletter
\newcolumntype{B}[3]{>{\boldmath\DC@{#1}{#2}{#3}}c<{\DC@end}}
\makeatother
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

% Options for algorithm2e
\DontPrintSemicolon

% Titlepage 
\subject{Master's Thesis}
\title{Reinforcement Learning for Navigating Particle Swarms by Global Force}
\author{Matthias Konitzny}
\date{\today}
\publishers{\textbf{Institute of Operating Systems and Computer Networks\\ Algorithms Group\\ Prof. Dr. S\'andor Fekete}\\
\vspace*{2em}
Supervisor:\\
Prof. Dr. S\'andor Fekete \par
Dominik Krupke \par}

% Math Operators
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\maketitle

\frontmatter

\cleardoublepage
%\makebackpage[plain]%[<plain/info/addressinfo>]


% statement of originality
\thispagestyle{plain} % no header
\vspace*{7cm}
\centerline{\bfseries Statement of Originality}
\vspace*{1em}
\noindent
This thesis has been performed independently with the support of my supervisor/s.
To the best of the author's knowledge, this thesis contains no material previously
published or written by another person except where due reference is made in the text.

\par
  \bigskip\noindent Braunschweig, \today \par
  \vspace*{10mm}
  \hfill\hrulefill
\cleardoublepage

% abstract
\thispagestyle{plain} % no header
\centerline{\bfseries Abstract}
\vspace*{1em}
\noindent
Since the early years of computer science, navigation tasks were heavily explored. Weather it is about planning optimal routes, exploring spaces or avoiding obstacles, most tasks have been found to be exceptionally hard to solve optimally. Therefore even modern computers struggle with the optimal solution of larger instances for those problems. By sacrificing optimal results for speed, the traditional approach was to find approximation algorithms which ideally guarantee some performance bound and yield usable results in a relatively short time. 

In this work we want to focus on a specific problem: The navigation of particles by a single global force - also known as tilt problem. The particles must be navigated to a given goal position through a maze-like environment. Instead of using traditional approximation algorithms, we use reinforcement learning (RL) to navigate the particles. We explore, how recent RL techniques perform under different settings and compare the results to multiple state-of-the-art approximation algorithms.
\cleardoublepage

\microtypesetup{protrusion=false}
\tableofcontents
\cleardoublepage

\listoffigures
\cleardoublepage

\listoftables
\cleardoublepage

\listofalgorithms
\cleardoublepage

\microtypesetup{protrusion=true}


\mainmatter

\include{introduction}

\include{targeted_drug_delivery}

\include{deep_learning}

\include{reinforcement_learning}

\include{implementation}

\include{evaluation}

\include{conclusion}

\bibliographystyle{abbrv}
\bibliography{sigproc}

\appendix

\include{baselines_lab_config}

\end{document}