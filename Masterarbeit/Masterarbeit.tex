\documentclass[%
  a4paper,%
  11pt,% <10pt, 9pt>
  style=print,
  %sender=bottom,
  blue,% <orange, green, violet>
  %rgb, <cmyk>
  %mono,
  bibliography=totoc,
  nexus,
  lnum,
  extramargin,
  table
  ]{tubsbook}

\usepackage[utf8x]{inputenc}
\usepackage[english]{babel}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{svg}
\usepackage[hidelinks]{hyperref}
\usepackage{amsmath}
\usepackage[makeroom]{cancel}
\usepackage[ruled, linesnumbered, algosection]{algorithm2e}
\usepackage{listings}
\usepackage{subcaption}
\usepackage{threeparttable}
\usepackage{dcolumn}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{booktabs}
\usepackage{pythonhighlight}
\newcolumntype{L}{D{.}{.}{2,2}}
\makeatletter
\newcolumntype{B}[3]{>{\boldmath\DC@{#1}{#2}{#3}}c<{\DC@end}}
\makeatother
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}

% Options for algorithm2e
\DontPrintSemicolon

% Titlepage 
\subject{Master's Thesis}
\title{Reinforcement Learning for Navigating Particle Swarms by Global Force}
\author{Matthias Konitzny}
\date{\today}
\publishers{\textbf{Institute of Operating Systems and Computer Networks\\ Algorithms Group\\ Prof. Dr. S\'andor Fekete}\\
\vspace*{2em}
Supervisor:\\
Prof. Dr. S\'andor Fekete \par
Dominik Krupke \par}

% Math Operators
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\maketitle

\frontmatter

\cleardoublepage
%\makebackpage[plain]%[<plain/info/addressinfo>]


% statement of originality
\thispagestyle{plain} % no header
\vspace*{7cm}
\centerline{\bfseries Statement of Originality}
\vspace*{1em}
\noindent
This thesis has been performed independently with the support of my supervisor/s.
To the best of the author's knowledge, this thesis contains no material previously
published or written by another person except where due reference is made in the text.

\par
  \bigskip\noindent Braunschweig, \today \par
  \vspace*{10mm}
  \hfill\hrulefill
\cleardoublepage

% abstract
\thispagestyle{plain} % no header
\centerline{\bfseries Abstract}
\vspace*{1em}
\noindent
Since the early years of computer science, navigation tasks were heavily explored. Weather it is about planning optimal routes, exploring spaces or avoiding obstacles, most tasks have been found to be exceptionally hard to solve optimally. One example is the navigation of distributed particles through a maze-like environment to a given goal position using a global force for control. While traditional approximation algorithms provide results in reasonable time, recent studies have shown that the use of reinforcement learning (RL) can compute significantly better solutions for individual instances.

In this work, we further explore and improve these reinforcement learning strategies. By introducing a novel continuous reward and optimizing the training procedure, we are able to decrease learning times by up to 95\% while still outperforming previous RL agents. These improved training conditions also allow us to train agents, which learn more generalized strategies and can perform navigation tasks for random goal positions. Additionally, we investigate the the performance of RL agents in settings which simulate real-world challenges. These environments include physical particle behavior as well as noisy observations and actions. We can show that RL agents are able to perform even under these new and more challenging conditions.  

\cleardoublepage

\microtypesetup{protrusion=false}
\tableofcontents
\cleardoublepage

\listoffigures
\cleardoublepage

\listoftables
\cleardoublepage

\listofalgorithms
\cleardoublepage

\microtypesetup{protrusion=true}


\mainmatter

\include{introduction}

\include{targeted_drug_delivery}

\include{deep_learning}

\include{reinforcement_learning}

\include{implementation}

\include{evaluation}

\include{conclusion}

\bibliographystyle{abbrv}
\bibliography{sigproc}

\appendix

\include{baselines_lab_config}

\end{document}