
% Define listing style for yaml
\newcommand\YAMLcolonstyle{\color{black}\bfseries\small}
\newcommand\YAMLkeystyle{\color{black}\mdseries\small}
\newcommand\YAMLvaluestyle{\color{green}\mdseries\small}

\makeatletter

% here is a macro expanding to the name of the language
% (handy if you decide to change it further down the road)
\newcommand\language@yaml{yaml}

\expandafter\expandafter\expandafter\lstdefinelanguage
\expandafter{\language@yaml}
{
  keywords={true,false,null,y,n},
  keywordstyle=\color{darkgray}\bfseries,
  basicstyle=\YAMLkeystyle,                                 % assuming a key comes first
  sensitive=false,
  comment=[l]{\#},
  morecomment=[s]{/*}{*/},
  commentstyle=\color{purple}\ttfamily,
  stringstyle=\YAMLvaluestyle\ttfamily,
  moredelim=[l][\color{orange}]{\&},
  moredelim=[l][\color{magenta}]{*},
  moredelim=**[il][\YAMLcolonstyle{:}\YAMLvaluestyle]{:},   % switch to value style at :
  morestring=[b]',
  morestring=[b]",
  literate =    {---}{{\ProcessThreeDashes}}3
                {>}{{\textcolor{red}\textgreater}}1     
                {|}{{\textcolor{red}\textbar}}1 
                {\ -\ }{{\mdseries\ -\ }}3,
}

% switch to key style at EOL
\lst@AddToHook{EveryLine}{\ifx\lst@language\language@yaml\YAMLkeystyle\fi}
\makeatother

\newcommand\ProcessThreeDashes{\llap{\color{cyan}\mdseries-{-}-}}


\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    xleftmargin=15pt
}

\lstset{style=mystyle}

\chapter{Implementation} \label{chp:Implementation}
In this chapter we will take a look at different parts of the implementation. We will begin by discussing some general implementation notes in Chapter \ref{sec:ImplementationNotes} and the continue to present our experimentation environment called \textit{baselines lab} in Section \ref{sec:BaselinesLab}. We then talk about the particle simulation environment, including different scenarios from the original implementation of Huang et al. in Section \ref{sec:MazeEnvironment}.

\section{General Implementation Notes} \label{sec:ImplementationNotes}
Before talking about the actual implementation we want to talk a little bit about our general design choices. As a programming language we chose to use Python 3 \cite{van2011python, pythonWebsite}. Python is an interpreted high-level programming language with itself is written in C. Even though it is interpreted and thus may be slower than languages like C or C++, the easy integration of high-performance libraries which also scale into distributed systems make applications developed in python very fast. In the past years almost every ML library is developed for python making Python an ideal choice for data-science and machine learning projects.

The popularity of machine learning has lead to the developed of a number of libraries which support the creation and training of artificial neural networks. Modern libraries also especially leverage the computing power of the GPU to accelerate the computation of large matrix operations. Many of the libraries support more or less the same functionality, but may have a very different style of how certain things can be achieved. Currently the two most popular libraries for machine learning are PyTorch \cite{paszke2019pytorch} and Tensorflow \cite{abadi2016tensorflow}, we decided to use the latter. This decision was made in conjunction with our second big backend library which extends Tensorflow for reinforcement learning: Stable-Baselines \cite{stable-baselines}. Stable baselines offers high-quality implementations of many RL algorithms which are partially forked from the OpenAI Baselines project \cite{baselines}. While Google offers their own reinforcement library under the name TF-Agents \cite{TFAgents} we found that it currently is in a too early development state and thus not stable enough for the course of this thesis.

Since Python offers the easy integration of additional libraries we also make use of a number of additional libraries, most notably:
\begin{itemize}
    \item OpenAI Gym \cite{openAIgym} for a standardized environment creation as well as the integration of the Atari environments for testing purposes.
    \item OpenCV \cite{opencv_library} for image preprocessing.
    \item Numpy \cite{oliphant2006guide} for fast matrix computations.
    \item Optuna \cite{akiba2019optuna} for automated hyperparameter search.
\end{itemize}

\section{Baselines Lab} \label{sec:BaselinesLab}
To test out various combinations of reinforcement learning algorithms, their settings with different environments and environment settings we need a flexible and easy configurable program which also gives us the possibility to monitor the performance and create evaluations of trained models. While there already exist some projects which offer some of these features we did not find a project which was fully suited for our needs. The stable-baselines co-project baselines zoo \cite{rl-zoo} had to few options for the configuration of algorithms and while other libraries like slm-lab \cite{kenggraesser2017slmlab} did fit our need for configurability, they did not offer enough reinforcement learning algorithms. Therefore we decided to build our own lab environment on top of stable-baselines which we call \textit{Baselines Lab}.

\subsection{Basic Features} \label{sec:blFunctions}
In this section, we want to go over the basic features of Baselines Lab. We will start with an overview of the main functionality and then give a little more detail on some of the most important features. 

When starting the lab, we have to choose between one of the three different lab modes:

\begin{enumerate}
    \item The \textit{train} lab mode can be used to train a new model.
    \item The \textit{enjoy} lab mode can be used to watch a model in action and/or evaluate its performance.
    \item The \textit{search} lab mode can be used to perform an automated hyperparametersearch. We will talk more about the search mode in Section \ref{sec:blSearch}.
\end{enumerate}

When starting a session, we also need to specify a \textit{lab configuration file} which can be written in JSON or YAML. Let us take a look at the easiest form of a config file which specifies all mandatory parameters:

\begin{figure}[h]
    \lstinputlisting[language=yaml]{figures/implementation/simple_config.yml}
    \caption[Basic lab configuration file]{Basic lab configuration file in YAML.}
    \label{fig:BasicLabConfig}
\end{figure}


We can see that the basic lab configuration only requires a handful of parameters which define which RL algorithm should be used in conjunction with which neural network, which environment and for how long the model should be trained. In general there are three basic categories for the parameters defined by the three main keywords: 

\begin{enumerate}
    \item The \textit{algorithm} keyword specifies all parameters related to the used reinforcement learning algorithm. The parameters which can be set are dependent on the used algorithm and correspond directly to the parameters of the stable-baselines documentation \cite{stable-baselines-docs}. The only exception to this is the policy, which defines the used neural network. The policy type - corresponding to its registered class - is defined under the \textit{name} keyword. Additional policy arguments can be directly specified under the \textit{policy} keyword.
    \item The \textit{env} keyword specifies all parameters which are directly related to the environment. The only mandatory parameter is the name of the environment corresponding to its registered gym entry. The env keyword allows for a lot of parameters regarding observation preprocessing like frame stacking or normalization. We included a full list in Appendix TODO. 
    \item  The \textit{meta} keyword describes all meta parameters for the training process. For example we can define how often the model should be saved or evaluated, which random seed should be used or how many times the experiment should be repeated. For a full list of available arguments, see Appendix TODO.
\end{enumerate}

Independent of the used lab mode, the same configuration file can be used. However some functions may depend on certain configurations. If we want to inspect an already trained model, we must specify a log directory under "meta" when training. This will result in models being saved periodically during training at the given location. The latest or best models can then be inspected in enjoy mode by just starting the lab with the same configuration file. 

\paragraph{Automated Saving and Loading.}
We already mentioned that it is possible to automatically save and load models, but we want to describe the procedure a little bit more in depth. The first question is what needs to be serialized when training the model. The answer to this depends on the used configuration, since it may include components which need to be saved additionally to the parameters of neural network. For example parameters if we normalized the observations of the environment using a running mean, we need to save that running mean, otherwise our model would not be able to perform after loading it. A single checkpoint therefore may contain multiple files depending on the current configuration.

If a log directory location is specified the lab will create a log directory with a current timestamp. Models (and additional components) will be periodically saved during training to a subdirectory called \textit{checkpoints}. On default, the lab keeps the last 5 checkpoints and the model which had the best performance during training. This behavior can also be changed (see Appendix TODO).

The checkpoints created during training can be used in enjoy mode or if we want to continue training in the future. To continue training, we just add a \texttt{trained\_agent} keyword to the algorithm configuration and specify weather we want to load the \textit{last} or the \textit{best} checkpoint. In enjoy mode the best checkpoint is selected on default. To change this behavior we can specify a \texttt{-{}-type} command line argument and set it to \textit{last}.

\paragraph{Automated Evaluation}
Models are automatically evaluated during training. This evaluation is done periodically to determine the current model performance. Evaluation can be done in three different modes depending on the desired tradeoff between evaluation accuracy and speed:

\begin{enumerate}
    \item \textbf{Fast.} Fast evaluation is not really doing a dedicated evaluation run. Instead the current results are computed by looking at the average reward and episode length over the last 100 training episodes. This procedure is very fast because the results are already computed during training, but may be inaccurate and also only represents the training behavior which might be substantially different depending on the used RL algorithm (e.g. for DQN).
    \item \textbf{Normal.} Normal evaluation uses a dedicated vectorized evaluation environment with 32 parallel instances. Because each instance is reset independently this evaluation type still might produce a small bias towards short episodes, which slightly improves results if short episodes are good and slightly worsens results if short episodes are bad. Nevertheless we found that the results do not deviate to much from the performance when executing the same model on a single environment and thus the results can be used to compare different models.
    \item \textbf{Slow.} Slow evaluation uses a dedicated evaluation environment with a single instance. This might take a lot of time during training, but produces very accurate results. 
\end{enumerate}

Evaluation can also be done outside of the train lab mode, by specifying the \texttt{-{}-evaluate} command line argument for the enjoy lab mode. The argument expects a number of episodes to test and uses the normal evaluation mode on default. If we are interested in completely accurate results we can additionally specify the \texttt{-{}-strict} option. Evaluation in enjoy mode automatically saves the evaluation results to the training log directory. 

\paragraph{Monitoring}
Monitoring the training process is very important in all machine learning areas. By logging all kinds of values we are able to determine errors in the implementation, select good hyperparameters and overall control the quality of the training process. Luckily, Tensorflow comes with an integrated monitoring system called Tensorboard which allows us to inspect the neural network and log values during training. These values can then be plotted in real time by starting the Tensorboard application (which gets automatically installed with Tensorflow). Figure \ref{fig:TensorboardExample} shows an example of the Tensorboard visualization, which can be accessed with a web browser.

\begin{figure}[ht]
    
    \begin{center}
        \includegraphics[clip, width=0.95\columnwidth]{figures/implementation/Tensorboard.png}
    \end{center}
    
    %\vspace*{-6pt}
    \caption[Tensorboard Example]{Example for a Tensorboard visualization.}
    \label{fig:TensorboardExample}
    %\vspace*{-12pt}
  \end{figure}

Stable Baselines already integrates Tensorboard for extended logging and visualization. Dependent on the chosen RL algorithm, Stable Baselines will log internal values like the average action advantage or the discounted reward. With Baselines Lab we extend these logging capabilities and now also include episode length and average episode reward for both training and evaluation. We also included a metric for steps per second which helps to monitor execution performance. For special modules like RND (see Section \ref{sec:blRND}) we also include module specific plots like intrinsic reward. The lab configuration is also included in the Tensorboard log file.

Tensorboard logs are created automatically at the given log directory location if the \texttt{tensorboard\_log} parameter of the algorithm is set to true.

\paragraph{Extension of Stable Baselines Functions.}
Additional to the extended monitoring, we also extended some other functions of Stable Baselines:

\begin{itemize}
    \item \textit{Learning Rate Schedules.} Stable Baselines includes an inconsistent system of learning rate schedules which are only available for a number of algorithms. In Baselines Lab we included linear and piecewise learning rate schedules for the PPO, SAC and TD3 implementations. For information on how these schedules can be used we included an example in Appendix TODO.
    \item \textit{Flexible Neural Network Creation.} Stable Baselines only allows to flexibly create dense neural networks. We extended the network base classes to also include a fully configurable network class which can be defined via config files. For an example see Appendix TODO.
\end{itemize}


\subsection{Random Network Distillation Module} \label{sec:blRND}
Since the Stable Baselines implementations do not offer any form of curiosity reward we needed to implement our own version. Huang et al. have shown, that the RND curiosity produces superior results in comparison with the original idea of an ICM module, so we decided to implement the RND reward only. 

\begin{figure}[ht]
    \begin{center}
    %\resizebox{0.95\columnwidth}{!}{%
    \begin{tabular}{c}
    \includegraphics[clip, height=5cm]{figures/implementation/rnd_pong_episode_reward.pdf} \\
    \includegraphics[clip, height=5cm]{figures/implementation/rnd_pong_episode_intrinsic_reward.pdf} \\
    \includegraphics[clip, height=5cm]{figures/implementation/rnd_pong_episode_length.pdf} \\
    
    \end{tabular}
    %}%
    \end{center}
    %\vspace*{-12pt}
    \caption[RND on Pong]{Learning curves for the verification experiment of the curiosity reward wrapper on the Atari game Pong.}
    \label{fig:RNDPong}
    %\vspace*{-12pt}
  \end{figure}


Instead of implementing the curiosity reward as an extension of PPO (like in the original implementation) we decided to implement RND as a wrapper for the environment. This wrapper contains an internal replay buffer from which we randomly draw samples to train the predictor network. As default, we train the predictor network with the same frequency as the actor network. The replay buffer contains samples from the past four training periods. As an example, if we train 256 steps on 16 environments between each training period of the actor, the replay buffer will have a size of $256 \times 16 \times 4$ samples. We also perform four optimization epochs per training step. The networks used for the target and predictor network have the same structure as in the original implementation. 

The implementation of the RND module as a wrapper allows us to use the curiosity reward signal independent of the RL algorithm. We verified our implementation by training an PPO agent to play the Atari game "Pong" using curiosity reward only and no end-of-episode signal. Figure \ref{fig:RNDPong} shows the learning curves for intrinsic and extrinsic reward, as well as the episode length. We can see, that the agent does not optimize for extrinsic reward - this is expected, since we removed that reward signal - and instead continuously improves the intrinsic reward by increasing the episode length. The agent therefore learns to play with its enemy in Pong instead of against him. This behavior was also observed by Burda et al.\cite{burda2018exploration}.

The development of the curiosity wrapper was done in conjunction with the development team of Stable Baselines, and the implementation has been staged for a future release of Stable Baselines 3 \cite{stable-baselines-intrinsic}.

\subsection{Automated Hyperparametersearch} \label{sec:blSearch}
The choice of optimal hyperparameters is very important in any machine learning setting. As we saw in Chapter \ref{chp: RLOverview}, most RL algorithms introduce multiple new parameters, with some of them having great influence on the training performance. These parameter must be tune additional to the already existing hyperparameters like the learning rate or other optimizer specific values, an observation preprocessing pipeline and eventual environment parameters (e.g. for reward generation). Tuning all these parameters together is especially hard when because every training run may take hours before we can determine if the current configuration is better or worse than any other configuration. To support our decisions when tuning these parameters, we therefore need the possibility of an automated hyperparametersearch. 

\begin{figure}[ht]
    \lstinputlisting[language=yaml]{figures/implementation/simple_search_config.yml}
    \caption{Basic search lab configuration file.}
    \label{fig:BasicSearchConfig}
\end{figure}

To efficiently search for hyperparameters in an ML context, a number of algorithms have been developed in the past. Modern optimizer frameworks often allow to choose between a number of different of these algorithms. For Baselines Lab we choose to integrate Optuna \cite{akiba2019optuna} as it combines a number of up-to-date algorithms with great configurability. In Optuna there are mainly two classes of important algorithms: Samplers and Pruners. Samplers are used to choose from a set of hyperparameters and pruners are used to decide which trial should be canceled early (pruned). While both classes can be used with simple algorithms (e.g. a random value sampler with a median pruner), in Optuna we also can choose from more advanced algorithms. For example sampling can be done using the tree-structured parzen estimator algorithm \cite{bergstra2011algorithms} or pruning can be done via successive halving \cite{karnin2013almost} which both showed to improve performance.

In Baselines Lab it is easy to configure which sampler or pruner to use and which parameters should be tuned. In Figure \ref{fig:BasicSearchConfig} we included an example lab configuration to demonstrate how hyperparametersearch works. The shown section for the \textit{search} keyword can be appended to the lab configuration from Figure \ref{fig:BasicLabConfig} and is only used if the file is started with the \texttt{search} lab mode. We can see that we can define how many trials should be run via the \texttt{n\_trials} keyword and that we can define a sampler and a pruner method. Each pruner can receive its individual parameters by specifying them under the \texttt{pruner} keyword. For example the \textit{halving} pruner get configured such that each trial will run for at least 4000 timesteps before it can be pruned.

Depending on the chosen RL algorithm, Baselines Lab comes with a predefined set of parameter ranges which will automatically be sampled. Any parameter defined in the normal algorithm or env section will not be sampled during the parametersearch. If we want to explicitly give a range for parameters, we can redefine them under search/algorithm or search/env with a sample method and the according choices. More details can be found in the Optuna documentation \cite{optuna-docs} and in Appendix TODO.

\subsection{Additional Features} \label{sec:blAdvanced}
Baselines Lab offers a bunch of additional functions which are designed to help monitoring the training process, find problems with the learning algorithm and help with result presentation:

\begin{itemize}
    \item \textit{Plots.} Baselines Lab can automatically create plots of the tensorboard training data. To create the plots either specify \texttt{plot: true} under the \texttt{meta} keyword or use the \texttt{-{}-plot} command line argument in enjoy mode. The default plots contain train and evaluation data for reward and episode length, but it is possible to plot additional data by specifying their tensorboard tags. For more information see Appendix TODO.
    \item \textit{Video Creation.} Baselines Lab can automatically create videos of  the environment. The environment can either be rendered like it is displayed in enjoy mode (rendered for human eyes) or the environment can render the observations which are created for the agent. Both options are only available in enjoy mode and can be enabled with the command line arguments \texttt{-{}-video} or \texttt{-{}-obs-video} respectively.
    \item \textit{Email Notification.} Baselines Lab can automatically send email notifications to send updates about the training progress. This feature is only available on Linux and requires \textit{mailx} to be installed. To send email notifications we have to specify a mail address via the \texttt{-{}-mail} command line argument. 
\end{itemize}

\section{The Particle Maze Environment} \label{sec:MazeEnvironment}
Since most RL algorithms require millions of steps of training before they can solve a problem, we need a very efficient implementation for our environment. At the same time we are interested in a modular design, since we want to experiment with rewards, random goal positions, random mazes (polyominos) or even different particle models. In Section \ref{sec:MazeImplementation} we present our implementation of the basic maze environment and its modular components. In the following sections we then explain in detail how these components work, beginning with Section \ref{sec:MazeReward} where we will talk about reward generation. We then talk about extensions of the original environment to bring the simulations closer to reality with the addition of physical particles or unprecise observations in Section \ref{sec:ExtendedMaze}. Finally we will talk about random instance generation in Section \ref{sec:RandomInstanceGeneration}.

\subsection{Basic Implementation} \label{sec:MazeImplementation}
We designed the implementation for the maze environment around two key ideas: First, the environment should run as fast as possible, because the agent will need to play hundreds or thousands of episodes. Designing the environment as lightweight as possible will therefore save a lot of time during training. Second, we need an environment which is configurable with our lab configuration file system in its main aspects. This means having the possibility to dynamically load instances, define rewards and even particle behavior. For the implementation we therefore decided to build the maze environment as a modular system, which allows easy configuration and extension of the existing architecture. 

Figure \ref{fig:MazeBaseDesign} shows a simplified overview of the architecture of the environments with its main component \textit{MazeBase}. The modules which can be integrated into the MazeBase class all inherit from three interfaces and therefore can be divided into three groups:

\begin{enumerate}
    \item \textit{Instance Generators.} Instance generators load or create instances. Depending on the generator, an instance can be just created once and then returned for every future episode (e.g. if we want to load a specific instance) or the instance can be randomly generated. We will talk more about random instance creating in Section \ref{sec:RandomInstanceGeneration}.
    \item \textit{Reward Generator.} Reward generators create rewards after each step and define when the episode is over. We will talk in detail about reward shaping in Section \ref{sec:MazeReward}.
    \item \textit{Step Modifiers.} Step modifiers define the behavior of the particles. This behavior might be very simple like in the original maze implementation of Huang et al. where each particle moves exactly one step into the direction given by the action, but can be more complex. We will talk about advanced particle behavior in Section \ref{sec:ExtendedMaze}. 
\end{enumerate}

\begin{figure}[ht]
    
    \begin{center}
        \includegraphics[clip, trim=10px 10px 10px 10px, width=0.9\columnwidth]{figures/implementation/maze_base_design.pdf}
    \end{center}
    
    %\vspace*{-6pt}
    \caption[Implementation Design for the Maze Environment]{Implementation Design for the Maze Environment.}
    \label{fig:MazeBaseDesign}
    %\vspace*{-12pt}
  \end{figure}

All components of the maze environment are generally designed with performance in mind. When implementing particle motions and reward generations, we minimized the use of python loop statements and instead solved most of the computations by utilizing NumPy \cite{oliphant2006guide} functions.

\subsection{Reward Shaping} \label{sec:MazeReward}
Rewards are a fundamental component of reinforcement learning. For RL algorithms the difficulty of a problem can largely depend on how much information the reward signal provides during training. Problems become especially complicated when dealing with very sparse rewards (e.g. just a single reward at the end of an episode). When designing a reward signal for a problem, it is therefore desirable to include as much information as possible into the reward signal to obtain faster and better learning results (except for cases where we are interested in an artificially hard problem like in RL algorithm research). Providing much information with the reward signal can include a risk though: The reward must be unbiased and should not limit the solution space the algorithm can come up with. If we reward specific strategies, the RL algorithm will just replicate that strategy and not learn anything new. 

Keeping in mind that we want to create an unbiased reward signal, we can use the simple distance to the goal position as a measure for the reward signal. Since pixel perfect navigation is hard for RL algorithms, we want to allow slight inaccuracy and define a small area around the exact goal position in which we accept the solution. We then can measure the distance to the goal position for each particle and create two distinct reward signals from this measure:

\begin{itemize}
    \item \textit{Total cost reward} is generated based on the sum of distances $d_{total}$ of all particles to the goal position. A positive total cost reward therefore indicates overall improvement. This reward may also be generated by calculating the average cost $d_{avg}$ instead of the total cost.
    \item \textit{Max cost reward} is generated based on the particle with the greatest distance to the goal position $d_{max}$ only. This reward may differ significantly from the total cost reward, since it strongly punishes left behind particles. 
\end{itemize}

As we mentioned in Section \ref{sec:TDDRL}, Huang et al. used both, the maximum and the average cost reward signal. Instead of giving a continuous reward signal, they defined 4-8 subgoals for both these values and only rewarded the agent if it was able to reduce the distance for the average or maximum particle below one of the subgoal thresholds. An example for this method is shown in Table \ref{tab:OriginalRewards}.

\begin{table} [ht]
    \begin{center}
        \begin{tabular}{|c|c|c|}
            \hline
            Average Cost & Reward & Completed \\
            \hline
            <10 & 8 & False \\
            <20 & 8 & False \\
            <40 & 4 & False \\
            <80 & 4 & True \\
            <120 & 2 & True \\
            $\vdots$ & $\vdots$ & $\vdots$ \\
            \hline
        \end{tabular}
    \end{center}
    \caption[Original Reward Example]{Example for the original reward system. By reaching a certain goal for a reward category (e.g. average cost) the agent is rewarded with a certain reward. Each reward can only be obtained a single time \cite{huang2019}.} \label{tab:OriginalRewards}
\end{table}


The sparseness of this reward signal may significantly increase the complexity of the problem. Also this reward is not flexible, since the reward subgoals were handcrafted for each individual instance. In order to investigate the impact of different rewards, we propose a number of reward components and test their impact on the performance in Section TODO. 

\paragraph{Discrete Reward.}
We implemented a reward which works similar to the original reward, but with a dynamic number of subgoals. We also drastically increased the number of subgoals, to make the reward less sparse. Instead of rewarding the agent at 4-8 subgoals (which in the original implementation were all given to the agent if its solution was already close to the goal position) we dynamically compute a number of subgoals depending on the size of the maze and distribute these subgoals evenly. This way the agent already receives reward at early stages of the training process. We estimate the number of subgoals by dividing the maximum distance any particle can be away from the goal position by 2. Subgoals which are easier to reach (the first subgoals) generate a smaller reward than later ones.

\paragraph{Continuous Reward.}
To reduce sparseness to a minimum, we also implemented continuous reward. The continuous reward does not define any subgoals and instead directly rewards the agent by the change in distances for both, the maximum and the total cost metric. In this case it is important to either use the average cost metric or scale both the total distance metric and the maximum cost metric to $[0, 1]$, otherwise the weight of the total distance metric is much larger than the weight of the max cost metric. There is a second aspect to using scaled total distance reward though and that is reward consistency: Larger instances would automatically generate more reward for the agent if the reward is directly dependent on the distance between particles and the goal position. If we want to explore agents which can solve randomly generated instances, we need a reward which is always on the same scale for an episode. We call the the continuous reward normalized, if both the total cost and the maximum cost metric are scaled to $[0, 1]$ (so the reward is bounded to $[0, 2]$). 

One problem when creating the continuous reward signal only based on the change in the particle distance is, that the same observation does not always create the same amount of reward. Remember when we talked about the idea of a baseline in Section \ref{ssec:ImprovingPG}? The problem was, that in most situations a high or a low reward does not tell us if the agents performance was good, because the starting situation for the agent might be better or worse than average just by chance. Therefore modern RL algorithms already account for these fluctuations by estimating how good the current state is. For optimal performance, we therefore must design a reward signal, which is consistent for the state of the environment - rewarding the agent even if it has just been lucky. 

To achieve this, we need an estimate for the starting cost for both the maximum and total (or average) cost metric, which is independent of the actual particle positions. For the total cost metric, we can use the total distance of each possible particle position to the goal position and for the maximum cost metric we use the maximum distance a single particle can be away from the goal position. For each step of training, the continuous reward is then calculated by subtracting the current value of each metric from the corresponding value in after the last step. 

Generating totally continuous reward may discourage the agent to gather particles which got stuck at some point, if it has to move other particles away from the goal position, because it may receive large amounts of negative reward in the process. We therefore propose an extension to the continuous reward signal which cuts off negative rewards and returns zero reward for all steps which would otherwise result in a negative reward.

\paragraph{Time Penalty.}
While the basic reward does encourage the agent to bring all particles to the goal position, there is no pressure on the agent to use a minimum amount of time. It will receive the same amount of reward independent of the total time it needs until all particles reach the goal. To add a time constraint, we can add a time penalty to the reward signal, which is given out at each step. The original implementation used a static time penalty of $-0.1$ reward per step. With our dynamic reward system this static time penalty does not work optimally for two reasons: First, the ratio between time penalty and the rest of the reward signal may not be the same for all environments, since larger environments now provide more reward due to particles being further away. Second, larger environments may need more time to solve and - like before - we are interested in a constant total value for the time penalty. The problem here is that we do not know beforehand what the optimal episode length will be. We therefore propose to estimate the optimal episode length by

\[ep\_len = d_{max} * \log d_{avg} * n\]

and the time penalty by $ep\_len^{-1}$. For discrete rewards, we scale this time penalty to match the sum of all rewards. 



\paragraph{Gathering Reward.}
Bringing the particles to the goal position requires to gather the particles in a small area or even in a single pixel. We therefore create an additional reward signal which is generated by calculating the number of unique particle positions in the current step. This reward signal can also be normalized to $[0, 1]$ by dividing it by the initial number of particles. Note that gathering reward might also be more complex by calculating the total pairwise particle distance, but this calculation is extremely costly and we therefore did not use this option.

\paragraph{Dynamic Episode Length.}

\paragraph{Final Reward.}
The final reward We tested multiple combinations of the reward signal in Section TODO, which include 



\subsection{Extended Models} \label{sec:ExtendedMaze}
Until now, we looked at the particle navigation problem from a very theoretical point of view. Each particle could be perfectly observed meaning we always had information about its pixel perfect position. We also were able to perfectly apply the same uniform transformation to each particle and the particles also were weightless and had near infinite friction, since they always directly stopped moving after each step. To investigate the performance of RL models under more real-life conditions, we therefore implemented two extensions which were tested stand alone and combined in Section TODO.

\paragraph{Introducing Error.}
When it comes to real world applications we always will have to deal with multiple sources of error. Sensors will always provide a certain amount of noise and actions might not be executed perfectly. To simulate various error sources we therefore introduce fuzzy observations and fuzzy actions.  


Additionally to noise, detecting the exact location of microscopic particles will always be inaccurate as particles may not be recognized correctly. We therefore generate fuzzy observations by using three possible sources of error:

\begin{itemize}
    \item \textit{Noise.} To simulate standard detection error, we add gaussian noise to the observation.
    \item \textit{False Negative Detection.} At each step, each particle has a small chance to be not be included in the observation.
    \item \textit{False Positive Detection.} At each step, particles with random positions may be added to the observation.
\end{itemize}

While particles in the theoretical model are directed using a uniform force, in reality particles will never move completely uniform. Slight changes in the magnetic field, collisions with other particles or fluctuations in the blood flow, may interfere with the navigation at any time. Also the generation of the magnetic fields may work with a slight delay and that delay might change over time. We therefore generate fuzzy actions using three sources of error.

\begin{itemize}
    \item \textit{Sticky Actions.} Each action has the probability to be executed again for the next step.
    \item \textit{Noisy Actions.} Each action may only affect a certain percentage of the particles.
    \item \textit{Random Actions.} Each particle may move randomly at each step.
\end{itemize}

We test all these sources of error and their influence on the performance in Section TODO.

\paragraph{Physical Particles.}
While error in actions and observations adds a lot to the realism of the simulation, we are still working with particles which directly change from one position to another. Instead we want to have particles which behave more like real particles would. Instead of directly changing the particle position, we instead accelerate particles. Particles themselves then have an internal speed vector which also influences their position for the following steps of the simulation. To simulate friction we divide this speed vector by a constant of $1.1$ for each step and set the speed to zero for very small speed values. The particle position is rounded for the observation after each step but kept at floating point precision for internal calculations.

One problem when simulating physical particles is that the computations for the environment get more complicated and training needs more time. This is especially problematic when calculating collisions with the maze. For the non-physical particles, each particle could at most move a single pixel in any direction. If the particle was blocked by the maze in that direction it would simply remain at its current position. However with physical particles, particles may actually move more than one pixel per step. In case of a collision, we would need to calculate the exact point of contact between the particle and the maze which would be costly. Instead we approximate the collision by halving the speed if the particle would cause a collision at full speed. The particle then does not move for the current step, but will keep half of its velocity for the next step, getting closer and closer to the wall. Note that this procedure might allow particles to move through walls if their velocity is large enough to cross the wall in a single step. However the size of our test instances makes it near impossible for a particle to be accelerated that much.

We investigate the performance of RL algorithms when dealing with physical particles in Section TODO.

\subsection{Random Instance Generation} \label{sec:RandomInstanceGeneration}
To train agents to solve randomized mazes, we need a generator which produces blood vessel like structures. To generate these structures we used an algorithm called \textit{rapidly-exploring random tree} (RRT) \cite{lavalle1998rapidly}. The algorithm was originally designed to efficiently search high-dimensional spaces by randomly generating a space-filling tree. The tree is generated by randomly proposing new points in the search space and connecting these new points to the nearest point of the search tree. Using the RRT algorithm, we create blood-vessel like instances in four steps:

\begin{enumerate}
    \item Generate a new RRT. A low bound on the maximum length for each new segment ensures a river-like structure of the tree. Figure \ref{fig:RRTTrees} (a) shows an example RRT tree with 250 nodes.
    \item We calculate the \textit{flow} to the starting node of the tree. Each leaf node generates a flow of one and propagates that flow back to the root node of the tree (see Figure \ref{fig:RRTTrees} (b)).
    \item Create random loops in the tree, by generating a set amount of random points and connecting them to the two nearest nodes of the tree. (see Figure \ref{fig:RRTTrees} (c))
    \item Draw the tree, using the square root of the flow value as width for each segment.
\end{enumerate}

\begin{figure}[ht]
    \begin{center}
    %\resizebox{0.95\columnwidth}{!}{%
    \begin{tabular}{cc}
    \includegraphics[clip, trim=40 40 40 40, height=4cm]{figures/implementation/rrt_base.png} &
    \includegraphics[clip, trim=40 40 40 40, height=4cm]{figures/implementation/rrt_base_flow.png} \\
    {\footnotesize (a) A simple RRT tree.} &
    {\footnotesize (b) Flow-based branch scaling.} \\
    \multicolumn{2}{c}{\includegraphics[clip, trim=40 40 40 40, height=4cm]{figures/implementation/rrt_base_loops.png}} \\
    \multicolumn{2}{c}{{\footnotesize (c) Adding random non-intersecting loops.}}
    \end{tabular}
    %}%
    \end{center}
    %\vspace*{-12pt}
    \caption[Random Instance Generation]{Creating vessel-like random instances with RRT trees.}
    \label{fig:RRTTrees}
    %\vspace*{-12pt}
  \end{figure}

When training with randomly created instances a goal is randomly chosen from the set of empty pixels. Additionally we buffer the randomly created instances and choose an already created instance with a probability of 95\% and only generate a new instance with a probability of 5\%, to avoid expensive recomputation at every environment reset.

\subsection{Integration of Algorithmic Approaches} \label{sec:AlgorithmIntegration}
To allow easy evaluation, we integrated an adapter which allows algorithmic strategies, which precompute all particle movements, to communicate with our step-by-step based environment and replay their decisions accordingly. By adapting code from \cite{becker2020} we have access to the SSP, DSP and MTE algorithms as shown in Section \ref{chp:TDD}. Evaluation results comparing the RL approach against the algorithmic approaches can be found in Section TODO.


